# Code-Generator-Code-Llama7b
This is a demo of code generator chatbot that is powered by the code llama-7b model. The model is fine tuned on code snippet and their corresponding natural language description.This allows model to generate code that is accurate and readable.

## Prerequisites

To run this demo, you will need the following:

* Python 3.6 or later
* The Langchain library
* The Gradio library

## Installation

To install the required dependencies, run the following command:

```
pip install langchain gradio
```

## Usage

To run the demo, clone this repository and run the following command:

```
python app.py
```

This will start a Gradio server on port 5000. You can access the demo by visiting http://localhost:5000 in your web browser.

## Code Explanation

The code for this demo is located in the `app.py` file. The following is a step-by-step explanation of the code:

1. Import the necessary libraries.
2. Define the custom prompt template.
3. Load the language model.
4. Create the LLM chain.
5. Define the bot function.
6. Start the Gradio server.

### Custom Prompt Template

The custom prompt template is defined using the `PromptTemplate` class. The template takes a query as input and returns a prompt that is used to generate code. The prompt is designed to be as specific as possible so that the model can generate accurate and relevant code.

### Load the Language Model

The language model is loaded using the `CTransformers` class. The `CTransformers` class is a wrapper for the Codellama-7B model. The model is loaded from a `.ggmlv3` file.

### Create the LLM Chain

The LLM chain is created using the `LLMChain` class. The `LLMChain` class is a wrapper for the language model and the prompt template. The chain is used to generate code by passing a query to the prompt template and then passing the output of the prompt template to the language model.

### Define the Bot Function

The bot function is defined using the `def` keyword. The function takes a query as input and returns a response. The response is generated by calling the `run()` method on the LLM chain. The `run()`
